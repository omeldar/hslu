\section{Farbe \& Licht}

Farbe kann man alltagssprachlich entlang von drei Dimensionen beschreiben:

\begin{itemize}
    \item Farbton (Hue)
    \item Sättigung/Farbstich (Saturation)
    \item Hellighekit (Brightness/Value)
\end{itemize}

\textbf{Licht als physikalische Grundlage (elektromagnetische Strahlung)}

Sichtbares Licht ist ein Ausschnitt des elektromagnetischen Spektrums. Unterschiedliche Lichtquellen (z.B. Sonne, Glühlampe, LED) haben unterschiedliche Spektren (spektrale Leistungsverteilung). Diese Unterschiede beeinflussen direkt, wie \enquote{dieselbe} Oberfläche unter verschiedenen Lichtquellen aussieht.

\textbf{Wie entsteht die Farbe einer Oberfläche (Reflexion/Absorption)?}

Eine Oerfläche \enquote{hat} keine Farbe an sich. Sie modifiziert das einfallende Licht: bestimmte Wellenlängen werden absorbiert, andere reflektiert. Das Auge empfängt dann das reflektierte Spektrum (Lichtquelle $\times$ Oberflächenreflexion), und daraus entsteht die wahrgenommene Farbe.

\clearpage
\section{Auge \& Wahrnehmung}

\begin{figure}[H]
    \img[width=0.55\textwidth]{figures/01/eye.png}
        {Menschliches Auge}
    \label{fig:eye}
\end{figure}

Auge als Kamera (vereinfachtes Modell):

\begin{itemize}
    \item Iris: Ring mit Muskeln, steuert die Öffnung um die grösse der Pupille zu beinflussen. Dies steuer wie viel Licht das Auge trifft (die \enquote{exposure}).
    \item Pupille: Öffnung, durch Iris geregelt (Lichtmenge)
    \item Linse: Fokussiert (Scharfstellen). Im Zusammenspiel mit der \enquote{Cornea} biegen sie das Licht so, dass es an den richtigen Ort gelangt.
    \item Retina: Hier werden die Licht-Photonen in \enquote{Elektrizität} umgewandelt. Es gibt Millionen von Photorezeptoren die dies aufnehmen.
    \item Photorezeptoren: Lichtaufnahme/Sensorik
\end{itemize}

Auf der Retina sitzen zwei Haupttypen von Photorezeptoren:

\begin{itemize}
    \item Stäbchen (rods): hauptsächlich Intensität/Helligkeit, sehr lichtempfindlich. Sie können ein einzelnes Photon erkennen. Stäbchen sehen aber kein Licht. Stäbchen sind vorallem Nachts aktiv, wenn wir wenig Umgebungslicht haben. Dadurch sehen wir im Dunkeln hauptsächlich in gray-scale.
    \item Zapfen (cones): Farbsehen. Wir haben viel weniger von denen (ca. 6 Millionen).
\end{itemize}

Wenn die Sonne untergeht geht die Farbe nicht weg. Unser System wechselt einfach von cones auf rods, damit wir überhaupt noch etwas sehen. Dadurch wird aber unser Farbsehen stark beeinträchtigt.

\subsection{Macula}

Das ist ein kleiner Ort wo die farb-wahrnehmenden cones sehr konzentriert sind (eng aufeinander). Das ist der einzige Ort im Auge der eine \enquote{hohe Auflösung} hat. Wenn wir herumschauen werden die Objekte auf die wir uns fokussieren möchten durch diesen Punkt \enquote{durchgescannt}. Alles andere rund um diesen Punkt in unserer Sicht nimmt das Auge nur sehr verschwommen und praktisch Farbenblind wahr. 

Dies fühlt sich nicht so an, da unser Hirn sich merkt was da vorher war (oder denkt sich dazu, was da sein müsste) und ergänzt es im Bild unserer Sicht.

\subsection{Optic disk}

An diesem Ort sind wir Menschen blind. Wir Menschen haben einen Blind-Spot. Wir sehen kein Loch in unserer Sicht, da das Hirn auch wieder das herumliegende als Basis nimmt, um diese Lücke zu füllen. (Vorstellbar wie ein AI die ein Loch in einem Bild füllt).

\subsection{Wie sehen wir Farbe? S/M/L-Zapfen}

\begin{figure}[H]
    \img[width=0.75\textwidth]{figures/01/cone-arten.png}
        {Wie sehen wir Farben}
    \label{fig:cone_farben}
\end{figure}

Wir Menschen sind drei-farb-sehend. Wir haben:

\begin{itemize}
    \item S-cones (440nm): Für kurzwelliges Licht (blaue Farbe).
    \item M-cones (530nm): Für mittelwelliges Licht (grüne Farbe).
    \item L-cones (560nm): Für langwelliges Licht (rote Farbe).
\end{itemize}

Es gibt unendlich viele Farben zwischen 400nm und 700nm. Wir sehen allerdings nur Farben an 3 Stellen (Blau, Grün, Rot). Unser Hirn komprimiert die Realität in 3 Farben, und versucht anhand einer Mischung dieser 3 Farben alle anderen Farben darzustellen.

Wieso hilft uns diese Limitierung der drei Farben im technischen Bereich?

Ein Monitor produziert nicht \enquote{gelbe Pixel}. Ein gelbes Pixel entsteht für uns im Hirn nur, wenn ein grünes und ein rotes sub-pixel nebeneinander leuchten. Wenn wir also das Cone Mosaic in Abbildung 2.2 ganz klein machen würden, würde unser Hirn dies als gelb sehen.

Monitore nutzen also einen \enquote{Bug} in unserer Biologie um die meisten Farben darzustellen.

\subsection{Helligkeitswahrnehmung: scotopic vs photopic}

Die Empfindlichkeit unterscheidet sich je nachdem, ob primär Stäbchen (scotopic vision) oder Zapfen (photopic vision) dominieren (z.B. Nacht vs Tag).

\subsection{Kontrastwahrnehmung \& Weber's Law}

Die Wahrnehmung von Kontrast hängt von der Grundhelligkeit ab (nicht nur vom absoluten Unterschied). Weber's Law wird hier als ungefähr konstantes Verhältnis formukliert:
$$
\Delta I / I = c
$$
und auch als log-Form (Differenz von log-Helligkeiten) notiert. Zusätzlich wird Kontrastempfindlichkeit ber Ortsfrequenzen (cycles/degree) und sogar Richtungsabhängigkeit dargestellt: Das visuelle System ist für bestimmte Muster-Frequenzen und Orientierungen empfindlicher als für andere.

\subsection{Mach Banding (Kanten-/Kontrastillusion)}

Bei stufenförmigen Helligkeitsverläufen sieht man oft scheinbare Über- und Unterbetonungen an den Grenzen (Mach Bands). Das ist wichtig in CG, weil Quantisierung oder schlecht gewählte Tonemaps solche Effekte sichtbar machen können.

\clearpage
\section{Farbräume \& Farbmodelle}

Licht ist physikalisch gesehen elektromagnetische Strahlung (Energie) in Form von Wellen. 

\begin{figure}[H]
    \img[width=0.5\textwidth]{figures/01/color_spectrum.png}
        {Farb-Spektrum}
    \label{fig:colorspec}
\end{figure}

Das ganze Licht-(Farben-)Spektrum beinhaltet auf einer Seite Gamma-Rays, X-Rays, Ultraviolet, was alles nicht sichtbar für uns (teilweise auch gefährlich für uns) ist mit extrem kurzen Wellenlängen.

Auf der anderen Seite des Spektrums haben wir z.B. Radio-Waves, die extrem lang sind, teilweise kilometer-lang.

Wir können nur einen kleinen Teil des Spektrums sehen (ca. $~400nm \text{ bis } ~700nm$). Der einzige physikalische Unterschied von Farben (in Echt) ist die Wellenlänge des Lichts. Unser \enquote{Rot und Blau} unterscheiden sich physikalisch rein in den Wellenlängen des Lichts, das ein Material in unser Auge reflektiert.

In der Realität sehen wir auch nie \enquote{nur eine Farbe des Licht-Spektrums}. Wir sehen immer einen Mix von allen Farben. Wir können aber mehr oder weniger einiger Wellenlänge sehen, was dann die Farbe beeinflusst.

\clearpage
\subsection{Spektrum von verschiedenen Lichtquellen}

\begin{multicols}{2}
    \begin{figure}[H]
        \img[width=0.54\textwidth]{figures/01/light_energy_spectrum.png}
            {Spektrum von verschiedenen Lichtquellen}
        \label{fig:light_energy_spectrum}
    \end{figure}
    
    \begin{figure}[H]
        \img[width=0.46\textwidth]{figures/01/color_spec_examples.png}
            {Spektrum von verschiedenen Lichtquellen - Beispiele}
        \label{fig:colorspec_examples}
    \end{figure}
\end{multicols}

Wenn wir uns die gelbe Linie \enquote{Daytime Sunlight} anschauen, sehen wir, dass die Wellenlängen des Lichts \enquote{relativ} gleichmässig verteilt sind über das ganze Spektrum hinweg. Da dadurch all unsere Farb-Rezeptoren (für alle 3 Farben) gleichzeitig \enquote{getriggered} werden, sehen wir \enquote{Daylight} als weisses licht.

Eine Tungston Light-Bulb ist gelblich/rötlich, da die Wellenlängen die sie ausstrahlt in diesem Bereich am prominentesten sind.

Wenn wir uns den \enquote{Ruby Laser} anschauen sehen wir, dass es nur ein sehr kleines Spektrum an Licht bei leicht unter $700\text{nm}$ ausstrahlt. In der Natur haben wir nie etwas, was so klar eine Farbe zugeordnet werde kann.

LEDs hingegen sind recht speziell. Bei der weissen LED sehen wir eine Kurve, die bei Blau stark zunimmt, bei grün schwächer ist, und bei gelb/rot wieder zunimmt. Diese Kurve entsteht auch ein wenig durch die Art wie das LEDs hergestellt werden. Eine weisse LED ist eigentlich eine blaue LED die in einem weissen Phosphor-Mantel bedeckt ist, welcher einiges der Energie absorbiert. Das weisse Licht das wir auf Monitoren sehen, ist eigentlich ein blau-gelbes licht, dass unser Hirn als \enquote{weiss} aufnimmt.

\clearpage
\subsection{Interaktion von Licht und Material}

\begin{multicols}{2}
    \begin{figure}[H]
        \img[width=0.54\textwidth]{figures/01/wie_entsteht_farbe.png}
            {Interaktion: Licht und Objekte}
        \label{fig:wie_entsteht_farbe}
    \end{figure}
    
    \begin{figure}[H]
        \img[width=0.46\textwidth]{figures/01/color_signal.png}
            {Farbe eines Objekts}
        \label{fig:color_signal}
    \end{figure}
\end{multicols}

Objekte selbst können wir nur sehen, da Licht in unser Auge (oder digital Kamera) trifft.

Wenn wir en Objekt mit einer Farbe sehen, hat nicht dieses Objekt direkt diese Farbe. Das Objekt hat Eigenschaften, die dazu beitragen, das gewisse Wellenlängen stärker oder schwächer absorbiert/reflektiert werden. Durch diese Eigenschaften (und die Eigenschaften der Lichtquelle) entsteht ein \enquote{Color signal}, dass wir dann sehen.

Eine Erdbeere ist für uns rot. Physikalisch ist die Oberfläche einer Erdbeere eine Art chemischer Filter für kurzwellige elektromagnetische Strahlung. Wenn blaues Licht auf die Erdbeere trifft, wird diese Energie stark absorbiert. Dieses blaue Licht wird durch das Absorbieren in Hitze umgewandelt. Das gleiche passiert bei \enquote{grünem Licht}. Wenn rotes Licht die Erdbeere trifft, \enquote{sagt} die Erdbeere: \enquote{Ich brauche dieses Licht nicht} und reflektiert es. Dadurch sehen wir die Erdbeere als rot. 

Das \enquote{Color Signal}, dass wir sehen, wird bestimmt durch $\text{Illumination} \cdot \text{Reflectance} = \text{Color Signal}$

Wenn wir also z.B. eine Erdbeere in einen Raum stellen, der nur durch ein klar blaues Laser-Licht beleuchtet wird, wird die Erdbeere all das blaue Licht absorbieren und wirkt auf uns schwarz. Die Erdbeere würde schwarz aussehen. Ohne die roten (hohen) Wellenlängen hat die Erdbeere kein rotes Licht zu reflektieren, dadurch kann sie auf uns nicht rot wirken.

\clearpage
\subsection{CIE Normalfarbtafel}

\begin{multicols}{2}
    \begin{figure}[H]
        \img[width=0.50\textwidth]{figures/01/farben.png}
            {CIE Normalfarbtafel}
        \label{fig:farben}
    \end{figure}
    
    \begin{figure}[H]
        \img[width=0.40\textwidth]{figures/01/cie_flach.png}
            {CIE Flach}
        \label{fig:cie_flach}
    \end{figure}
\end{multicols}

Diese Normalfarbtafel stellt alle Farben dar, die ein durchschnittlicher Mensch sehen kann. Alle Farben ausserhalb dieser Grafik ist nicht sichtbar für Menschen.

Die Oberseite dieser Farbtafel (Blau zu Grün zu Rot) an der Kante entlang, zeigt die Spektralfarben (Farben des Regenbogens in der höchsten Sättigung). Hier sind die \enquote{reinen Grundfarben} die wir aufnehmen können (Blau, Grün, Rot).

Wenn wir uns von aussen nach innen bewegen in dieser Farbtafel, werden die Farben \enquote{gemischt mit Weiss}. Sie verlieren an Sättigung, da die anderen Grundfarben dazukommen, was die Hauptfarbe \enquote{verweisslicht}. Ganz im inneren sehen wir den weissen Punkt.

Ganz unten in dieser Farbtafel sehen wir eine gerade Linie (die Purpurlinie). Wir sehen in einem Regenbogen nie Magenta oder Hotpink. Diese Farben existieren im sichtbaren Spektrum gar nicht. Es gibt keine Wellenlängen im sichtbaren Farbspektrum, dass diese Farben erzeugt. Diese Farben entstehen rein, da unser Hirn verwirrt wird.

Der untere Rand der CIE-Farbtafel ist nicht gekrümmt, sondern eine gerade Linie, die das reine Blau links mit dem reinen Rot rechts verbindet: die Purpurlinie (line of purples). Denk an einen Regenbogen: Wir sehen Violett, Blau, Grün, Gelb, Rot. Aber wir sehen kein Magenta, kein Pink im Regenbogen. Das liegt daran, dass Purpur (wirklich gesättigtes Magenta) im Spektrum nicht als einzelne Wellenlänge existiert. Es entsteht im Gehirn (sozusagen aus Verwirrung).

Wenn wir blaues Licht sehen (kurze Wellenlängen, die S-Zapfen feuern) und rotes Licht (lange Wellenlängen, die L-Zapfen feuern), dann feuern normalerweise auch die Grünrezeptoren (M-Zapfen), wenn wir etwas \enquote{dazwischen} sehen. Wenn die Grünrezeptoren aber still bleiben, dann registriert das Gehirn zwei Enden des Spektrums gleichzeitig, aber nichts in der Mitte. Das Gehirn \enquote{denkt}: Rot und Blau liegen auf gegenüberliegenden Seiten der Realität... das sollte so nicht passieren, ich muss eine Brücke für diese Lücke erfinden. 

Darum existiert diese gerade Linie (Purpurlinie) nur in unserem Kopf: Sie überbrückt diese mathematische Lücke zwischen Rot und Blau. Es ist eine wunderschöne Halluzination.

\subsection{Überblick Farbmodelle (Kategorien)}

\begin{itemize}
    \item Hardware-orientiert: RGB (Monitore), CMY/CMYK (Druck), YIQ,YUV (Tv/Video)
    \item Intuitiv/benutzerorientiert: HSV, Munsell, CIELab
\end{itemize}

\textbf{RGB / sRGB (additiv)}

\begin{itemize}
    \item Additives Modell: Farbe $C = (r, b, c)$ mit $r, g, b \in [0, 1]$.
    \item sRGB als konkreter Standardfarbraum wird im Vergleich im CIE-Kontext gezeigt.
\end{itemize}

\begin{figure}[H]
    \img[width=0.5\textwidth]{figures/01/colorsforprinting.png}
        {Geräte und Farben}
    \label{fig:colorsforprinting}
\end{figure}

\subsubsection{CMY(K) (subtraktiv, Druck)}

Subtraktive Mischung mit Cyan, Magenta, Yellow (Komplementärfarben zu RGB). Zusammenhang zu RGB (idealisiert):
$$
(C, M, Y) = (1 - R, 1 - G, 1 - B)
$$

\subsubsection{HSV/HSB (intuitiv)}

\begin{itemize}
    \item Dimensionen: Hue (Farbton), Saturation (Sättigung), Value/Brightness (Intensität, Helligkeit)
    \item Geometrische Interpretation: Kegel oder Zylinder, V entlang z (0 schwarz, 1 weiss), S als Radius, Hue als Winkel (z.B. rot $0°$, grün $120°$, blau $240°$),
\end{itemize}

\subsubsection{YUV / YCbCr (Video, Luma/Chroma-Trennung)}

\begin{itemize}
    \item Idee: Zerlegung in Luminanz (Y) und zwei Farbdifferenzsignale (U, V); kompatibel zu Schwarzweiss-TV.
    \item Begründung für Praxis: U, V werden oft heruntergesampelt, weil das visuelle System empfindlicher auf Luminanzdetails als auf Farbdetails reagiert.
    \item Formeln (RGB $\rightarrow$ YUV), u. a.:
    \begin{itemize}
        \item $Y = 0.299R + 0.587G + 0.114B$
        \item alternative Schreibweisen für U,V und Rüktransformation zu RGB sind angegeben.
    \end{itemize}
    \item YCbCr wird als Beispiel erwähnt/illustriert.
\end{itemize}

\subsubsection{CIELab (wahrnehmungsnäher)}

\begin{itemize}
    \item Achsen: L (Helligkeit), a (rot $\rightleftarrows$ grün), b (gelb $\rightleftarrows$ blau).
    \item Ziel/Interpretation: \enquote{gleiche Abstände} sollen gleich empfundene Farbunterschiede bedeuten
\end{itemize}

\subsubsection{Grauwerte aus RGB (Luma-Gewichtung)}

Für die Umrechnung wird die Luma-Formel genutzt:
$$
Y = 0.299R + 0.587G + 0.114B
$$

\section{Dimensions of Visual Sensation (Qualitätsdimensionen von Bild/Video)}

\begin{itemize}
    \item Auflösung (HD, 4K/UHD, \enquote{retina})
    \item Frame Rate (z.B. 24Hz vs 48Hz)
    \item Dynamic Range: sichtbar vs Display; HDR und Tone Mapping (linear vs nichtlinear).
    \item Bit Depth / Quantisierung: Beispiele zeigen Artefakte bei wenigen Graustufen; auch \enquote{1 bit/pixel vs 8 bit/pixel}
    \item Depth: stereo 3D, autostereo, holographic
\end{itemize}

\textbf{HDR-Formate}

\begin{itemize}
    \item HDR10: Wide-gamut, 10-bit, Transfer Function, hoher Kontrast.
    \item HDR10+: Metadata per frame
    \item Dolby Vision: ähnlich, aber bis zu 12-bit Farbtiefe und 4K-Display erwähnt.
\end{itemize}
